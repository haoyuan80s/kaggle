{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate\n",
    "from keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Gloable vars ##\n",
    "RANDOM_SEED = 100\n",
    "TRAINNING_PATH = 'data/train.csv'\n",
    "EMBEDDING_DIMENSION = 300\n",
    "MAX_LEN = 100\n",
    "STD = 0.16  # std of embedding matrix entries\n",
    "# EMBEDDING_PATH = 'data/crawl-300d-2M.vec'\n",
    "EMBEDDING_PATH = 'data/glove.twitter.27B.25d.txt'  \n",
    "\n",
    "\n",
    "def get_coefs(word, *arr):\n",
    "    \"[good 2 4 3] --> good, [2,3,4]\"\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def build_embeddings_dict(embedding_path):\n",
    "    \"Build embedding dictionary.\"\n",
    "    with open(embedding_path) as f:\n",
    "        return dict(get_coefs(*line.strip().split(' ')) for line in f)\n",
    "\n",
    "def initilize_embedding_matrix(word_num, embedding_dimension):\n",
    "    \"Initilize embedding matrix as zeros.\"\n",
    "    return np.random.randn(word_num+1, embedding_dimension)*STD  # 0.16 = std of embedding matrix entries\n",
    "\n",
    "def build_embedding_matrix(word_index, embedding_path):\n",
    "    \"Build embedding matrix.\"\n",
    "    embeddings_dict = build_embeddings_dict(embedding_path)\n",
    "    embedding_dimension = len(embedding_index['good'])\n",
    "    embedding_matrix = initilize_embedding_matrix(word_num, embedding_dimension)\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            pass  # if miss give a random vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building embedding matrix\n"
     ]
    }
   ],
   "source": [
    "trainning_data = pd.read_csv('data/train.csv')\n",
    "x_train = trainning_data.comment_text\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\n",
    "word_num = len(tokenizer.word_index)\n",
    "y_train = trainning_data.target\n",
    "embedding_matrix = build_embedding_matrix(tokenizer.word_index, EMBEDDING_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(\n",
    "    embedding_matrix,\n",
    "    LSTM_UNITS = 128,\n",
    "    DENSE_HIDDEN_UNITS = 512\n",
    "):    \n",
    "    words = Input(shape=(MAX_LEN,))\n",
    "    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n",
    "    x = SpatialDropout1D(0.3)(x)\n",
    "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "    hidden = concatenate([\n",
    "        GlobalMaxPooling1D()(x),\n",
    "        GlobalAveragePooling1D()(x),\n",
    "    ])\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    result = Dense(1, activation='sigmoid')(hidden)\n",
    "    \n",
    "    model = Model(inputs=words, outputs=result)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, x_val, y_tr, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1443899 samples, validate on 360975 samples\n",
      "Epoch 1/1\n",
      " 763904/1443899 [==============>...............] - ETA: 5:22 - loss: 0.2524"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 512\n",
    "model.fit(x_tr,y_tr,batch_size=BATCH_SIZE,validation_data=[x_val, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
